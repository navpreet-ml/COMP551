# -*- coding: utf-8 -*-
"""Miniproject 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r2fl_-CoKWA_vUzbxDAuYly9oXduDR0B
"""

import numpy as np
from string import punctuation
import torch
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.nn.utils import clip_grad_norm_
import matplotlib.pyplot as plt
import statistics
import math

from google.colab import drive
drive.mount('/content/gdrive')  #data is saved in google drive, read the files from google drive
with open('/content/gdrive/My Drive/Colab Notebooks/pos.txt', 'r', encoding="latin-1") as f:
    pos_reviews = f.readlines()
with open('/content/gdrive/My Drive/Colab Notebooks/neg.txt', 'r', encoding="latin-1") as f:
    neg_reviews = f.readlines()
    
from gensim.models import KeyedVectors #Copied this from internet
# Creating the model
lookup_table = KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

def clean(reviews):
    clean_reviews = []
    for review in reviews:
        clean_review = ''
        for c in review:
            if c not in punctuation:
                clean_review = clean_review + c
            else:
                clean_review = clean_review + ' '
        clean_reviews.append(clean_review)
    return clean_reviews

def vectorize(total_reviews):
    total_reviews = clean(total_reviews)
    xtrains = np.zeros((10662,53,300))
    j = 0
    missing_words = []
    missing_words_count = 0
    total_words_count = 0
    missing_dict = {}
    for i, reviews in enumerate(total_reviews):
        examine = reviews.split()
        for j, word in enumerate(examine):
            try:
                xtrains[i][j] = lookup_table[word]
                total_words_count += 1
            except KeyError:
                if word in list(missing_dict.keys()):
                    xtrains[i][j] = missing_dict[word]
                else:
                    vector = np.random.normal(loc=-0.0036, scale=0.118, size=(300, )) # ==================== random setup
                    xtrains[i][j] = vector
                    missing_dict[word] = vector
                    missing_words.append(word)
                    missing_words_count += 1
                    total_words_count += 1
    print('missing_words_count', missing_words_count, 'total_words_count', total_words_count)
    return xtrains

def init_weights(m):
    if type(m) == nn.Linear:
        #torch.nn.init.normal_(m.weight, mean=-0.0036, std=0.118)
        #nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('relu'))
        stdv = 1. / math.sqrt(m.weight.size(1))
        torch.nn.init.normal_(m.weight, -stdv, stdv)
  
def datadim(X, Y):
    # 30000x2, labels['Category'].values 30000x1 np.array
    labels = torch.tensor(Y, dtype=torch.long)#.type('torch.DoubleTensor')
    # train_images 30000x64x64, torch.Size([30000, 1, 64, 64])
    sentences = torch.tensor(X)[:, None, :, :]#.type('torch.DoubleTensor')
    labels = labels.to(device) ###############################################
    sentences = sentences.to(device) #########################################
    return torch.utils.data.TensorDataset(sentences, labels)

def train(epoch):
    network.train()
    for batch_idx, (data, target) in enumerate(train_loader): # batchsize = 64, batch_idx 0 - 124
        optimizer.zero_grad()
        output = network(data)
        loss = criterion(output, target)
        loss.backward()
        clip_grad_norm_(network.parameters(), 3.0)
        optimizer.step()
#        if batch_idx % log_interval == 0:
#            print('Train Epoch: {} ({:.3f}%) \tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(train_loader), loss.item()))
    train_losses.append(loss.item())
    train_counter.append(epoch)

def test(epoch):
    network.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = network(data)
            test_loss += criterion(output, target, size_average=False).item()
            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()
    test_loss /= len(test_loader.dataset)
    test_losses.append(test_loss)
    if epoch % 10 == 0:
      print('Test set (epoch {}): Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(epoch, test_loss, correct, len(test_loader.dataset), 100. * float(correct) / len(test_loader.dataset)))
    return 100. * float(correct) / len(test_loader.dataset)

total_reviews = pos_reviews + neg_reviews
x = np.float32(vectorize(total_reviews))
y = np.array([1]*len(pos_reviews) + [0]*len(neg_reviews))

class SentClass(nn.Module): # epoch = 100 => 93.98%
    def __init__(self):
        super(SentClass, self).__init__()

        self.conv0 = nn.Conv2d(1, 100, kernel_size=(2, 302), padding=1)
        self.conv1 = nn.Conv2d(1, 100, kernel_size=(6, 310), padding=5)
        
        self.dropout1 = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(200, 2)

        
    def forward(self, x):
        x0 = F.relu(F.max_pool2d(self.conv0(x), (54, 1)))      
        x1 = F.relu(F.max_pool2d(self.conv1(x), (58, 1)))
        x0 = x0.view(-1, 100)
        x1 = x1.view(-1, 100)
        
        x = torch.cat((x0, x1), 1)
        
        x = self.dropout1(x)
        x = self.fc1(x)
        
        return F.softmax(x)

split = 0.1
batchsize = 50
log_interval = 50

learning_rate = 1
ro = 0.9
epslon = 1e-06
n_epochs = 50
k = 12

cross_validation_accuracy = []

for i in range(k):
    network = SentClass()
    #####
    network.apply(init_weights)
    #####
    network.cuda() ###############################################################
    criterion = F.nll_loss
    
    #optimizer = optim.SGD(network.parameters(), lr=learn, momentum=p)
    #optimizer = optim.Adadelta(network.parameters(), lr = learning_rate, rho = ro, eps=epslon)
    optimizer = optim.Adadelta(network.parameters(), lr = learning_rate)
    #optimizer = optim.RMSprop(network.parameters())
    #optimizer = optim.Adagrad(network.parameters())
    #optimizer = optim.Adam(network.parameters())
    rd = np.random.randint(10000)
    
      
    # Load data / train_images 30000x64x64, train_labels 30000x2
    xtrains, xtests, ytrains, ytests = train_test_split(x, y, test_size=split, random_state=rd)
    train_loader = torch.utils.data.DataLoader(datadim(xtrains, ytrains), batch_size=batchsize, shuffle=True)
    test_loader = torch.utils.data.DataLoader(datadim(xtests, ytests), batch_size=batchsize, shuffle=True)
    
    train_losses = []
    train_counter = []
    test_losses = []
    test_counter = [i for i in range(n_epochs + 1)]
    
    print('{}-fold cross validation.'.format(i+1), 'Random State: ' + str(rd))
    test(0)
    for epoch in range(1, n_epochs + 1):
        train(epoch)
        acc = test(epoch)
    cross_validation_accuracy.append(round(acc,3))
    print()

    fig = plt.figure()
    plt.plot(train_counter, train_losses, color='blue')
    plt.scatter(test_counter, test_losses, color='red')
    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')
    plt.xlabel('Number of Training Epoches (Run: {})'.format(i+1))
    plt.ylabel('Negative log Likelihood Loss')
    fig
    plt.show()
    print()

cross_validation_accuracy.sort()
print('Split:', split,', batch size:', batchsize,', # of epoches:', n_epochs,', learning rate:', learning_rate,', rho:', ro,' eps:', epslon)
print('Collection of {}-fold cross validation:'.format(k-2), cross_validation_accuracy)
print('Mean: ', statistics.mean(cross_validation_accuracy[1:-1]))
print('Standard deviation: ', statistics.stdev(cross_validation_accuracy[1:-1]))
cross_validation_accuracy